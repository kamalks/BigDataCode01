# Trusted Big Data ML with Occlum for production
This director is for production, and build occlum runable instance first.

You can see building command in [manually_build.yaml](https://github.com/intel-analytics/BigDL/blob/main/.github/workflows/manually_build.yml#L485) : bigdl-ppml-trusted-big-data-ml-scala-occlum-production.
It will build image normally, and then run occlum-build to build occlum runable instance (by running occlum init and build first) in /opt/occlum_spark. The default configuration is:
```bash
-e SGX_MEM_SIZE=30GB \
-e SGX_THREAD=2048 \
-e SGX_HEAP=1GB \
-e SGX_KERNEL_HEAP=1GB \
```

final_name=`intelanalytics/bigdl-ppml-trusted-big-data-ml-scala-occlum-production:${TAG}-build`. But the final image size is too large because there are too many dependencies in this image.

### Pull production image from dockerhub and add self libs or source code.
```bash
docker pull intelanalytics/bigdl-ppml-trusted-big-data-ml-scala-occlum-production:2.3.0
```
1. enter image
```bash
# Clean up old container 
export container_name=bigdl-ppml-trusted-big-data-ml-scala-occlum-production 
sudo docker rm -f $container_name 

# Run new command in container 
sudo docker run -it \
        --net=host \
        --name=$container_name \
        --cpuset-cpus 3-5 \
        -e SGX_MEM_SIZE=30GB \
        -e SGX_THREAD=2048 \
        -e SGX_HEAP=1GB \
        -e SGX_KERNEL_HEAP=1GB \
        -e ENABLE_SGX_DEBUG=true \
        -e ATTESTATION=true \
        intelanalytics/bigdl-ppml-trusted-big-data-ml-scala-occlum-production:2.3.0 \
        bash 
```
2. Add python code into /opt/py-examples, add python libs in to /opt/python-occlum, add jars into $BIGDL_HOME/jars.
3. build runable image
```bash
bash /opt/run_spark_on_occlum_glibc.sh init 
```
4. commit and get runable image
```bash
docker commit $container_name $container_name-build
```

### Pull production-build image from dockerhub.

```bash
docker pull intelanalytics/bigdl-ppml-trusted-big-data-ml-scala-occlum-production:2.3.0-build
```

## Using BigDL PPML Occlum EHSM Attestation
Bigdl ppml use EHSM as reference KMS&AS, you can deploy EHSM following the [guide](https://github.com/intel-analytics/BigDL/tree/main/ppml/services/ehsm/kubernetes#deploy-bigdl-ehsm-kms-on-kubernetes-with-helm-charts)
We assume you have already set up environment and enroll yourself on EHSM.

In [start-spark-local.sh](https://github.com/intel-analytics/BigDL/blob/main/ppml/trusted-big-data-ml/scala/docker-occlum/production/start-spark-local.sh). Set `ATTESTATION` = true and modify `PCCL_URL`, `ATTESTATION_URL` to the env value you have set,
and modify `APP_ID`, `API_KEY` to the value you have get  when enroll, and then you can change `CHALLENGE` and
`REPORT_DATA` for attestation.

``` bash
#start-spark-local.sh
-e ATTESTATION=false \   set to true to start attestation.
-e PCCS_URL=https://PCCS_IP:PCCS_PORT \  PCCS URL, obtained from KMS services or a self-deployed one. Should match the format https://<ip_address>:<port>.
-e ATTESTATION_URL=ESHM_IP:EHSM_PORT \  URL of attestation service. Should match the format <ip_address>:<port>.
-e APP_ID=your_app_id \ The appId generated by your attestation service.
-e API_KEY=your_api_key \ The apiKey generated by your attestation service.
-e CHALLENGE=cHBtbAo= \ Challenge to get quote of attestation service which will be verified by local SGX SDK. Should be a BASE64 string. It can be a casual BASE64 string, for example, it can be generated by the command echo ppml|base64.
-e REPORT_DATA=ppml \ A random String to generator a quote which will be send to attestation service and use for attest. Default is ppml.
```
### Register and get policy_Id
1. Verify that the attestation server is trusted.
```bash
bash start-spark-local.sh verify
```
2. Register application on attestation server
```bash
bash start-spark-local.sh register
```
3. Get the policy_Id and save for running this occlum application.
```bash
policy_Id ${policy_Id}
```

Then set policy_Id and run application in docker or k8s.
